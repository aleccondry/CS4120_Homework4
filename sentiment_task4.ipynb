{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 4\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names & Sections\n",
    "----\n",
    "Names: __YOUR NAMES HERE__ (Write these in every notebook you submit. For each partner, write down whether you are a 4120 or a 6120 student.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Neural Networks (20 points)\n",
    "----\n",
    "\n",
    "Next, we'll train a feedforward neural net to work with this data. You'll train one neural net which takes the same input as your Logistic Regression model - a sparse vector representing documents as bags of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsubr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hsubr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sentiment_utils as sutils\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# you can experiment with having some Dropout layers if you'd like to\n",
    "# this is not required\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# if you want to use this again\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Van', 'Dien', 'must', 'cringe', 'with', 'embarrassment', 'at', 'the', 'memory', 'of', 'this', 'ludicrously', 'poor', 'film', ',', 'as', 'indeed', 'must', 'every', 'single', 'individual', 'involved', '.', 'To', 'be', 'honest', 'I', 'am', 'rather', 'embarrassed', 'to', 'admit', 'I', 'watched', 'it', 'from', 'start', 'to', 'finish', '.', 'Production', 'values', 'are', 'somewhere', 'between', 'the', 'original', 'series', 'of', \"'Crossroads\", \"'\", 'and', \"'Prisoner\", 'Cell', 'Block', 'H', \"'\", '.', 'Most', 'five', 'year', 'olds', 'would', 'be', 'able', 'to', 'come', 'up', 'with', 'more', 'realistic', 'dialogue', 'and', 'a', 'more', 'plausible', 'plot', '.', 'As', 'for', 'the', 'acting', 'performances', ',', 'if', 'you', 'can', 'imagine', 'the', 'most', 'rubbish', 'porno', 'you', 'have', 'ever', 'seen', '-', 'one', 'of', 'those', 'ones', 'where', 'the', 'action', 'is', 'padded', 'out', 'with', 'some', 'interminable', \"'story\", \"'\", 'to', 'explain', 'how', 'some', 'pouting', 'old', 'peroxide', 'blonde', 'boiler', 'has', 'come', 'to', 'be', 'getting', 'spit-roasted', 'by', 'a', 'couple', 'of', 'blokes', 'with', 'moustaches', '-', 'you', 'will', 'have', 'some', 'idea', 'of', 'the', 'standard', 'of', 'acting', 'in', \"'Maiden\", 'Voyage', \"'\", '.', 'Worse', 'still', ',', 'you', 'ca', \"n't\", 'even', 'fast', 'forward', 'to', 'the', 'sex', 'scenes', ',', 'because', 'there', 'are', \"n't\", 'any', '.', 'An', 'appallingly', 'dreadful', 'film', '.']\n",
      "['Its', 'spelled', 'S-L-A-S-H-E-R-S', '.', 'I', 'was', 'happy', 'when', 'the', 'main', 'character', 'flashed', 'her', 'boobs', '.', 'That', 'was', 'pretty', 'tight', '.', 'Before', 'and', 'after', 'that', 'the', 'movie', 'pretty', 'much', 'blows', '.', 'The', 'acting', 'is', 'like', 'E-list', 'and', 'it', \"'s\", 'shown', 'well', 'in', 'the', 'movie', '.', 'Not', 'to', 'mention', 'it', 'is', 'so', 'low', 'budget', 'that', 'Preacherman', 'and', 'Chainsaw', 'Charlie', 'are', 'played', 'by', 'the', 'same', 'person', '.', 'The', 'whole', 'movie', 'looks', 'like', 'it', 'was', 'shot', 'with', 'a', 'camcorder', 'instead', 'of', 'half', 'way', 'decent', 'film', '.', 'The', 'only', 'other', 'reason', 'I', 'liked', 'the', 'movie', 'was', 'because', 'Chainsaw', 'Charlie', 'and', 'Doctor', 'Ripper', 'were', 'funny', '.', 'They', 'said', 'many', 'stupid', 'things', 'that', 'made', 'me', 'laugh', '.', 'Other', 'than', 'that', 'if', 'you', 'see', 'this', 'movie', 'at', 'Blockbuster', 'do', 'everyone', 'a', 'favor', 'hide', 'it', 'behind', 'Lawnmowerman', '2', '.', 'Anybody', 'that', 'thinks', 'this', 'movie', 'is', 'good', 'should', 'be', 'mentally', 'evaluated', '.']\n",
      "['My', 'jaw', 'fell', 'so', 'many', 'times', 'watching', 'this', 'flick', ',', 'I', 'have', 'bruises', '.', 'Okay', ',', 'granted', ',', 'I', 'really', 'was', \"n't\", 'expecting', 'the', 'quality', 'of', ',', 'say', ',', 'The', 'Others', 'or', 'even', 'Thirteen', 'Ghosts', '(', 'the', 'new', 'one', ',', 'which', 'was', 'just', 'dreadful', 'and', 'is', 'still', 'head', 'and', 'shoulders', 'above', 'this', 'insanity', ')', '.', 'Someone', 'else', 'noted', 'the', 'thin', 'characters', '...', 'I', 'would', \"n't\", 'call', 'them', '``', 'thin', \"''\", '.', '``', 'Thin', \"''\", 'implies', 'there', 'might', 'be', 'something', 'to', 'them', '.', 'How', 'about', 'almost', 'non-existent', '?', 'In', 'no', 'particular', 'order', 'we', 'have', ':', 'The', 'Girl', 'Who', 'Will', 'Scream', ';', 'The', 'American', 'Who', 'Will', 'Figure', 'It', 'All', 'Out', ';', 'The', 'Macho', 'Guy', 'Who', 'Will', 'Just', 'Bull', 'Through', 'Everything', 'Until', 'He', 'Gets', 'Killed', ':', 'The', 'Wise', 'Black', 'Man', 'Who', 'Will', 'Die', 'Early', ';', 'The', 'Extra', 'Guy', 'Who', 'Is', 'There', 'To', 'Die', 'First', ';', 'The', 'Extra', 'Woman', 'Who', 'Is', 'There', 'To', 'Play', 'Tough', '.', 'That', \"'s\", 'it', '.', 'That', \"'s\", 'your', 'character', 'list', 'and', 'that', 'is', 'what', 'they', 'are', 'and', 'what', 'they', 'remain', 'from', 'beginning', 'to', 'end', '.', 'If', 'they', 'were', '``', 'thin', \"''\", 'they', 'might', ',', 'at', 'least', ',', 'change', 'a', 'little', 'bit', 'from', 'beginning', 'to', 'end', '.', 'But', 'they', 'do', \"n't\", '.', 'Well', ',', 'okay', ',', 'the', 'American', 'guy', 'decides', 'he', \"'s\", 'going', 'to', 'stay', 'with', 'the', 'fieldwork', 'at', 'the', 'end', 'and', 'the', 'Screaming', 'Girl', 'goes', 'back', 'to', 'wherever', 'she', 'came', 'from', '.', 'That', \"'s\", 'the', 'change', '.', 'Other', 'than', 'that', ',', 'they', 'all', 'act', 'according', 'to', 'their', 'assigned', 'roles', 'and', 'rarely', 'betray', 'any', 'real', 'emotion', 'when', 'they', 'finally', 'meet', 'up', 'with', 'the', 'menace.Now', ',', 'the', 'producers', 'get', 'props', 'for', 'an', 'original', 'menace', ',', 'I', 'will', 'say', '.', 'I', 'had', 'understood', 'the', 'story', 'was', 'going', 'to', 'be', '``', 'Tremors', \"''\", 'but', 'with', 'ants', 'instead', 'of', 'giant', 'worms', '.', 'I', 'give', 'the', 'writer', 'credit', ':', 'these', 'are', 'very', 'cool', ',', 'very', 'scary', 'ants', 'and', 'what', 'they', 'do', 'with', 'bones', 'is', 'excellent', '.', '(', 'The', 'first', 'time', 'the', '``', 'bone', 'snatcher', \"''\", 'appear', ',', 'I', 'admit', 'I', 'jumped', 'a', 'few', 'feet', '.', ')', 'Unfortunately', ',', 'the', 'very', 'cool', 'concept', 'becomes', 'Alien', 'in', 'the', 'Desert', 'very', 'quickly', '.', 'We', 'get', 'a', 'lot', 'of', 'commentary', 'on', 'ants', 'that', 'may', 'or', 'may', 'not', 'be', 'true', ',', 'but', 'we', 'do', \"n't\", 'get', 'much', 'of', 'the', 'mythology', 'on', 'which', 'the', 'menace', 'is', 'based', '.', 'And', 'we', 'get', 'every', 'monster', 'movie', 'clichÃ©', 'ever', 'made', '.', 'People', 'go', 'into', 'places', 'they', 'know', 'they', 'should', \"n't\", 'and', 'when', 'they', 'have', 'no', 'compelling', 'reason', 'to', '.', 'Moronic', 'characters', 'try', 'to', 'hinder', 'our', 'heroes', 'and', 'die', 'for', 'it', '.', 'One', 'character', 'does', 'double', 'duty', 'as', '``', 'scientist', 'who', 'does', \"n't\", 'want', 'to', 'kill', 'the', 'monster', 'but', 'study', 'it', \"''\", '.', 'A', 'Very', 'Cool', 'Gadget', 'is', 'introduced', 'only', 'so', 'the', 'American', 'can', 'tell', 'everyone', 'something', 'about', 'ants', 'that', ',', 'gee', ',', 'I', 'hope', 'everyone', 'knows', 'anyway', '.', 'Then', 'the', 'gadget', 'is', 'broken', '.', 'Our', 'heroes', 'run', 'out', 'of', 'the', 'one', 'thing', 'that', 'can', 'keep', 'the', 'menace', 'at', 'bay', '.', 'And', 'then', 'there', 'is', 'that', 'final', ',', 'annoying', 'moment', 'when', 'we', 'know', 'the', 'menace', 'is', 'still', 'with', 'us', '--', 'and', 'wonder', 'exactly', 'what', 'and', 'how', 'the', 'hey', 'the', 'hero', 'or', 'heroine', 'came', 'by', 'it', '.', 'It', 'completely', 'renders', 'everything', 'that', 'went', 'before', 'as', 'useless', 'and', 'false.Three', 'stars', 'for', 'the', 'cool', 'use', 'of', 'ants', 'and', 'bones', '.', 'Nothing', 'at', 'all', 'for', 'clichÃ©s', ',', 'clunky', 'dialogue', 'and', 'dim', 'bulb', 'characters', '.']\n",
      "['``', 'The', 'belief', 'in', 'the', 'Big', 'Other', 'as', 'an', 'invisible', 'power', 'structure', 'which', 'exists', 'in', 'the', 'Real', 'is', 'the', 'most', 'succinct', 'definition', 'of', 'paranoia', '.', \"''\", '\\x96', 'Slavoj', 'Zizek', 'This', 'is', 'a', 'review', 'of', '``', 'Marathon', 'Man', \"''\", 'and', '``', 'The', 'Falcon', 'and', 'the', 'Snowman', \"''\", ',', 'two', 'films', 'by', 'director', 'John', 'Schlesinger.Though', 'Hitchcock', 'and', 'Lang', 'brought', 'the', '``', 'conspiracy', 'thriller', \"''\", 'to', 'Hollywood', ',', 'the', 'genre', 'only', 'blossomed', 'in', 'the', 'late', '60s', 'and', '70s', ',', 'with', 'films', 'like', '``', 'The', 'Parallax', 'View', \"''\", ',', '``', 'Z', \"''\", ',', '``', 'Marathon', 'Man', \"''\", ',', '``', 'Capricorn', 'One', \"''\", ',', '``', 'The', 'Manchurian', 'Candidate', \"''\", ',', '``', 'Three', 'Days', 'of', 'the', 'Condor', \"''\", 'and', '``', 'All', 'The', 'President', \"'s\", 'Men', \"''\", '.', 'This', 'was', 'the', 'age', 'of', 'Vietnam', 'and', 'Watergate', ',', 'the', 'public', 'deeply', 'suspicious', 'of', 'all', 'political', 'leaders.The', 'genre', 'remained', 'quiet', 'in', 'the', '80s', 'and', 'early', '90s', ',', 'until', 'the', '``', 'X', 'Files', \"''\", 'TV', 'series', 'sprung', 'to', 'life', '.', 'With', 'taglines', 'such', 'as', '``', 'The', 'Truth', 'Is', 'Out', 'There', \"''\", 'and', '``', 'Trust', 'No', 'One', \"''\", ',', 'the', 'series', 'posited', 'a', 'world', 'of', 'vast', 'conspiracies', 'and', 'government', 'plots', ',', 'the', 'common', 'man', 'at', 'the', 'mercy', 'of', 'all', 'manners', 'of', 'ridiculously', 'elaborate', 'schemes', '.', 'The', 'only', 'way', 'out', 'of', 'the', 'maze', '?', '``', 'Fight', 'the', 'future', '!', \"''\", 'as', 'the', 'tagline', 'of', 'the', 'series', \"'\", 'final', 'season', 'proclaimed', '.', 'It', 'was', 'apparently', 'our', 'duty', 'to', 'trawl', 'through', 'the', 'labyrinth', 'of', 'information', ',', 'discovering', 'some', 'elusive', '``', 'truth', \"''\", 'that', 'ensured', 'our', 'own', 'freedom.This', 'trend', 'ended', 'with', 'the', 'boom', 'of', 'the', 'internet', ',', 'conspiracy', 'thrillers', 'now', 'giving', 'way', 'to', '``', 'conspiracy', 'documentaries', \"''\", '.', 'The', 'internet', 'generation', 'lapped', 'up', 'such', 'independent', 'documentaries', 'as', '``', 'Loose', 'Change', \"''\", 'and', '``', 'Zeitgeist', \"''\", ',', 'whilst', 'in', 'the', 'mainstream', 'Michael', 'Moore', 'titillated', 'his', 'audience', 'with', 'stuff', 'like', '``', 'Fahrenheit', '9/11', \"''\", '.', 'All', 'these', 'documentaries', 'believed', 'in', 'a', '``', 'secret', 'order', \"''\", ',', 'a', 'cabal', 'of', 'wealthy', 'politicians', 'and', 'businessmen', 'who', 'conspire', 'to', 'reduce', 'human', 'rights', 'and', 'enslave', 'the', 'world', '.', 'They', 'struggle', 'to', 'create', 'a', 'mono-myth', ',', 'linking', 'various', 'conspiracies', 'and', 'hidden', 'agendas', 'into', 'a', 'single', ',', 'all', 'encompassing', 'narrative', 'that', 'explains', 'the', 'purpose', 'and', 'point', 'and', 'future', 'of', 'everything.This', 'need', 'to', '``', 'streamline', 'narratives', \"''\", ',', 'to', 'make', 'them', 'more', '``', 'efficient', \"''\", ',', 'is', 'reflected', 'in', 'the', 'scientific', 'community', ',', 'who', 'battle', 'to', 'create', 'a', '``', 'Grand', 'Unification', 'Theory', \"''\", 'and', 'ultimately', 'a', '``', 'Theory', 'of', 'Everything', \"''\", ',', 'merging', 'everything', 'from', 'Quantum', 'Mechanics', 'to', 'Special', 'Relativity', 'into', 'one', 'giant', 'all', 'encompassing', 'formula.So', 'ultimately', ',', 'the', '``', 'conspiracy', 'thriller', \"''\", 'is', 'rooted', 'in', 'man', \"'s\", 'desire', 'to', 'have', 'control', '.', 'The', 'modern', 'subject', 'is', 'one', 'who', 'displays', 'outright', 'cynicism', 'towards', 'official', 'institutions', ',', 'yet', 'at', 'the', 'same', 'time', 'believes', 'in', 'the', 'existence', 'of', 'conspiracies', '(', 'an', 'unseen', 'Other', 'pulling', 'the', 'strings', ')', '.', 'This', 'apparently', 'contradictory', 'coupling', 'of', 'cynicism', 'and', 'belief', 'is', 'strictly', 'related', 'to', 'the', 'demise', 'of', 'the', 'big', 'Other', '.', 'Its', 'disappearance', 'causes', 'us', 'to', 'construct', 'an', 'Other', 'of', 'the', 'Other', '(', 'conspiracy', ')', 'in', 'order', 'to', 'escape', 'the', 'unbearable', 'freedom', 'its', 'loss', 'causes', '.', 'Conversely', ',', 'there', 'is', 'no', 'need', 'to', 'take', 'the', 'Big', 'Other', 'seriously', 'if', 'we', 'believe', 'in', 'an', 'Other', 'of', 'the', 'Other', '.', 'We', \"'re\", 'therefore', 'allowed', 'to', 'display', 'cynicism', 'and', 'belief', 'in', 'equal', 'measures', '.', 'Man', 'thus', 'seeks', 'to', 'assert', 'control', 'over', 'a', 'wayward', 'universe', ',', 'to', 'create', 'a', 'kind', 'of', 'paternal', 'babysitter', '(', 'be', 'it', 'God', ',', 'a', 'mathematical', 'formula', ',', 'a', 'conspiracy', 'theory', ',', 'an', 'explanation', 'for', 'violence/conspiracies/murder/war', 'etc', ')', 'who', 'provides', 'meaning', 'and', 'symbolic', 'order', '.', 'The', 'Big', 'Other', 'provides', 'reassurances', 'to', 'the', 'believer', '.', 'It', \"'s\", 'a', '``', 'lifestyle', 'choice', \"''\", ',', 'akin', 'to', 'religion', ',', 'in', 'which', 'his', 'place', 'in', 'the', 'world', 'is', 'dependent', 'on', 'sheer', 'irrationality', '.', 'The', 'problem', 'with', 'most', '``', 'conspiracy', 'thrillers', \"''\", ',', 'from', 'the', 'innocent', 'days', 'of', 'Hitchcock', \"'s\", '``', 'Topaz', \"''\", 'all', 'the', 'way', 'up', 'to', 'modern', 'fare', 'like', '``', 'The', 'Da', 'Vinci', 'Code', \"''\", ',', 'are', 'two', 'fold', '.', 'Firstly', ',', 'they', 'are', 'not', 'incorrect', 'in', 'suggesting', 'that', 'something', 'is', '``', 'wrong', \"''\", 'amongst', 'the', '``', 'elite', \"''\", 'or', '``', 'best', 'people', \"''\", ',', 'but', 'they', 'are', 'incorrect', 'in', 'individualizing', 'and', 'personalizing', 'processes', 'that', 'are', 'social', ',', 'collective', 'and', 'systemic', ',', 'an', 'approach', 'which', 'implies', 'that', 'it', 'is', 'just', 'a', 'question', 'of', 'personal', 'morality', 'rather', 'than', 'social', 'structures', '.', 'Secondly', ',', 'and', 'most', 'importantly', ',', 'these', '``', 'conspiracies', \"''\", 'ignore', 'the', 'fact', 'that', 'the', 'Big', 'Other', 'simply', 'does', \"n't\", 'exist', '.', 'There', 'is', 'no', 'symbolic', 'order', 'pulling', 'the', 'strings.Some', 'modern', '``', 'conspiracy', 'thrillers', \"''\", '(', '``', 'Eyes', 'Wide', 'Shut', \"''\", ',', '``', 'Existenz', \"''\", 'etc', ')', 'acknowledge', 'this', ',', 'with', 'their', 'untangleable', 'webs', 'of', 'lies', ',', 'accidents', ',', 'truths', 'and', 'half', 'truths', ',', 'nothing', 'ever', 'adding', 'up', ',', 'nothing', 'ever', 'making', 'sense', ',', 'the', 'real', 'and', 'the', 'hyperreal', ',', 'the', 'truth', 'and', 'the', 'desire', ',', 'all', 'blurred', ',', 'without', 'any', 'identifiable', 'ground', 'zero', ',', 'but', 'these', 'are', 'mostly', 'films', 'by', 'intellectual', 'directors.Compared', 'to', 'these', 'modern', '``', 'conspiracy', 'thrillers', \"''\", ',', '``', 'Marathon', 'Man', \"''\", 'and', '``', 'The', 'Falcon', 'and', 'the', 'Snowman', \"''\", 'are', 'positively', 'archaic', '.', '``', 'Marathon', 'Man', \"''\", 'is', 'a', 'about', 'a', 'grad', 'student', '(', 'Dustin', 'Hoffman', ')', 'who', 'gets', 'embroiled', 'in', 'his', 'big', 'brother', \"'s\", 'business', '(', 'Roy', 'Scheider', ')', ',', 'which', 'unfortunately', 'has', 'to', 'do', 'with', 'spies', ',', 'guns', ',', 'double', 'agents', ',', 'diamonds', 'and', 'evil', 'Nazi', 'dentists', '.', 'Scheider', 'is', 'suave', ',', 'Hoffman', 'is', 'excellent', 'and', 'Schlesinger', 'hits', 'us', 'with', 'some', 'neat', 'visuals', '(', 'the', 'reveal', 'of', 'the', 'Eiffel', 'tower', 'is', 'stunning', ')', ',', 'but', 'what', \"'s\", 'most', 'interesting', 'about', 'the', 'film', 'is', 'the', 'way', 'that', 'its', 'various', 'plot', 'lines', 'do', \"n't\", 'intersect', 'until', 'the', '1', 'hour', 'mark', '.', 'Even', 'then', ',', 'it', 'takes', 'a', 'further', 'half', 'hour', 'for', 'things', 'to', 'start', 'making', 'sense', '.', 'Unfortunately', ',', 'the', 'film', 'ends', 'with', 'a', 'clichÃ©d', 'showdown', 'between', 'the', 'villain', 'and', 'the', 'good', 'guy', ',', 'everything', 'neatly', 'resolved', 'and', 'explained', '.', '``', 'The', 'Falcon', 'and', 'the', 'Snowman', \"''\", 'is', 'a', 'bit', 'more', 'ambitious', '.', 'Sean', 'Penn', 'and', 'Timothy', 'Hutton', 'play', 'two', 'friends', 'who', 'sell', 'government', 'secrets', 'to', 'the', 'Soviet', 'Union', '.', 'Hutton', 'works', 'at', 'a', 'civil', 'defence', 'contractor', 'and', 'smuggles', 'information', 'out', 'of', 'his', 'office', 'and', 'into', 'the', 'hands', 'of', 'Penn', ',', 'a', 'small', 'time', 'drug', 'dealer', 'who', 'has', 'no', 'qualms', 'selling', 'to', 'the', 'KGB', '.', 'Penn', 'does', 'this', 'strictly', 'for', 'the', 'money', ',', 'whilst', 'Hutton', 'is', 'disillusioned', 'with', 'the', 'American', 'government', '(', 'particularly', 'its', 'attempt', 'to', 'depose', 'the', 'leader', 'of', 'Australia', ')', 'and', 'so', 'sells', 'the', 'secrets', 'strictly', 'because', 'he', 'hates', 'how', 'his', 'country', 'conducts', 'crimes', 'and', 'games', 'of', 'espionage', '.', 'In', 'other', 'words', ',', 'the', 'film', 'is', 'about', 'a', 'conspiracy', 'undertaken', 'as', 'a', 'response', 'to', 'conspiracies', '.', '``', 'Marathon', 'Man', \"''\", '\\x96', '7.9/10', '``', 'The', 'Falcon', 'and', 'the', 'Snowman', \"''\", '\\x96', '8/10', 'Aside', 'from', 'an', 'oddly', 'slapstick', 'car', 'crash', 'and', 'its', 'clichÃ©d', 'ending', ',', '``', 'Marathon', 'Man', \"''\", 'is', 'an', 'effective', 'thriller', ',', 'with', 'several', 'neat', 'scenes', '.', '``', 'The', 'Falcon', 'and', 'the', 'Snowman', \"''\", 'is', 'even', 'better', ',', 'Penn', 'turning', 'in', 'a', 'memorable', 'performance', '.']\n",
      "[0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\"\n",
    "\n",
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_tups = sutils.generate_tuples_from_file(DEV_FILE)\n",
    "\n",
    "\n",
    "print(train_tups[0][0])\n",
    "print(train_tups[0][1])\n",
    "print(train_tups[0][2])\n",
    "print(train_tups[0][3])\n",
    "\n",
    "print(train_tups[1][:4])\n",
    "\n",
    "# Shape of train_tups and dev_tups\n",
    "\n",
    "#    [ [ [...], [...], [...], [...], ..... 1600 length],\n",
    "#      [   0,     1,     1,     0,   ..... 1600 length ]]\n",
    "\n",
    "\n",
    "# you may use either your sparse vectors or sklearn's CountVectorizer's sparse vectors\n",
    "# you will experiment with multinomial and binarized representations later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_multi: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X_test_multi: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X_train_bi: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X_test_bi: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "y_train: [0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "y_test: [0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#train_flat is the list of all tokens from the training tuples\n",
    "train_flat = []\n",
    "for row in train_tups[0]:\n",
    "    train_flat.extend(row)\n",
    "\n",
    "#test_flat is the list of all tokens from the dev tuples\n",
    "test_flat = []\n",
    "for row in dev_tups[0]:\n",
    "    test_flat.extend(row)\n",
    "\n",
    "#All tokens from train and dev tuples\n",
    "test_flat.extend(train_flat)\n",
    "vocab = set(test_flat)\n",
    "\n",
    "reviews_train = train_tups[0]\n",
    "reviews_test = dev_tups[0]\n",
    "\n",
    "X_train_multi = sutils.featurize(vocab=vocab, data_to_be_featurized_X=reviews_train, binary=False, verbose=False)\n",
    "X_test_multi = sutils.featurize(vocab=vocab, data_to_be_featurized_X=reviews_test, binary=False, verbose=False)\n",
    "\n",
    "X_train_bi = sutils.featurize(vocab=vocab, data_to_be_featurized_X=reviews_train, binary=True, verbose=False)\n",
    "X_test_bi = sutils.featurize(vocab=vocab, data_to_be_featurized_X=reviews_test, binary=True, verbose=False)\n",
    "\n",
    "y_train = train_tups[1]\n",
    "y_test = dev_tups[1]\n",
    "\n",
    "print(f'X_train_multi: {X_train_multi}')\n",
    "print(f'X_test_multi: {X_test_multi}')\n",
    "print(f'X_train_bi: {X_train_bi}')\n",
    "print(f'X_test_bi: {X_test_bi}')\n",
    "print(f'y_train: {y_train}')\n",
    "print(f'y_test: {y_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 200)               6617000   \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6637201 (25.32 MB)\n",
      "Trainable params: 6637201 (25.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a feedforward neural network model\n",
    "# that takes a sparse BoW representation of the data as input\n",
    "# and makes a binary classification of positive/negative sentiment as output\n",
    "# you may use any number of hidden layers >= 1 and any number of units in each hidden layer (we recommend between 50-200)\n",
    "# you may use any activation function on the hidden layers \n",
    "# you should use a sigmoid activation function on the output layer\n",
    "# you should use binary cross-entropy as your loss function\n",
    "# sgd is an appropriate optimizer for this task\n",
    "# you should report accuracy as your metric\n",
    "# you may add Dropout layers if you'd like to\n",
    "\n",
    "# create/compile your model in this cell\n",
    "\n",
    "model_multi = Sequential()\n",
    "\n",
    "model_multi.add(Dense(units=200, activation='relu', input_dim=X_train_multi.shape[1]))\n",
    "\n",
    "model_multi.add(Dense(units=100, activation='tanh', input_dim=X_train_multi.shape[1]))\n",
    "\n",
    "\n",
    "# put in an output layer\n",
    "model_multi.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model_multi.summary()\n",
    "# call compile here\n",
    "\n",
    "model_multi.compile(loss='binary_crossentropy',\n",
    "                 optimizer='sgd',\n",
    "                 metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 200)               6617000   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6697601 (25.55 MB)\n",
      "Trainable params: 6697601 (25.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bi = Sequential()\n",
    "\n",
    "model_bi.add(Dense(units=200, activation='relu', input_dim=X_train_multi.shape[1]))\n",
    "\n",
    "model_bi.add(Dense(units=200, activation='tanh', input_dim=X_train_multi.shape[1]))\n",
    "\n",
    "model_bi.add(Dense(units=200, activation='sigmoid', input_dim=X_train_multi.shape[1]))\n",
    "\n",
    "model_bi.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model_bi.summary()\n",
    "\n",
    "model_bi.compile(loss='binary_crossentropy',\n",
    "                 optimizer='sgd',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many trainable parameters does your model have? Our model has 6697601 trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial model fit\n",
      "Epoch 1/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.6764 - accuracy: 0.5869\n",
      "Epoch 2/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.6369 - accuracy: 0.6681\n",
      "Epoch 3/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.5909 - accuracy: 0.6888\n",
      "Epoch 4/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.5726 - accuracy: 0.7169\n",
      "Epoch 5/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.5272 - accuracy: 0.7644\n",
      "Epoch 6/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.4985 - accuracy: 0.7887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1081c1512a0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your model\n",
    "# Felix's computer takes about 2 sec for 3 epochs\n",
    "# reports an accuracy of 0.78 at that point using the sgd optimizer\n",
    "\n",
    "# Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})\n",
    "# indicates you should change a list into a numpy array\n",
    "\n",
    "print('Multinomial model fit')\n",
    "model_multi.fit(np.array(X_train_multi), np.array(y_train), epochs=6, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binomial model fit\n",
      "Epoch 1/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.6961 - accuracy: 0.4894\n",
      "Epoch 2/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.6921 - accuracy: 0.5169\n",
      "Epoch 3/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.6910 - accuracy: 0.5394\n",
      "Epoch 4/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.6897 - accuracy: 0.5337\n",
      "Epoch 5/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.6878 - accuracy: 0.5619\n",
      "Epoch 6/6\n",
      "50/50 [==============================] - 2s 31ms/step - loss: 0.6852 - accuracy: 0.5694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1081c0acca0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Binomial model fit')\n",
    "model_bi.fit(np.array(X_train_bi), np.array(y_train), epochs=6, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 13ms/step\n",
      "First 5 assigned probabilities for multinomial model: [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "First 5 assigned labels for multinomial model: [1, 1, 1, 1, 1]\n",
      "Counter({1: 200})\n"
     ]
    }
   ],
   "source": [
    "# make a prediction on the dev set\n",
    "# then make a classification decision based on that prediction\n",
    "# predicting all examples takes < 1 sec on Felix's computer\n",
    "\n",
    "multi_labels = model_multi.predict(X_test_multi)\n",
    "\n",
    "multi_assigned_labels = [1 if y_hat_val > .5 else 0 for y_hat_val in multi_labels]\n",
    "print(\"First 5 assigned probabilities for multinomial model:\", multi_labels[:5])\n",
    "print(\"First 5 assigned labels for multinomial model:\", multi_assigned_labels[:5])\n",
    "\n",
    "print(collections.Counter(multi_assigned_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 8ms/step\n",
      "First 5 assigned probabilities for binomial model: [[0.49804437]\n",
      " [0.49342486]\n",
      " [0.49592173]\n",
      " [0.5168608 ]\n",
      " [0.4983251 ]]\n",
      "First 5 assigned labels for binomial model: [0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "bi_labels = model_bi.predict(X_test_bi)\n",
    "\n",
    "bi_assigned_labels = [1 if y_hat_val > .5 else 0 for y_hat_val in bi_labels]\n",
    "print(\"First 5 assigned probabilities for binomial model:\", bi_labels[:5])\n",
    "print(\"First 5 assigned labels for binomial model:\", bi_assigned_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6659 - accuracy: 0.5250\n",
      "Test multinomial model loss: 0.6658968329429626\n",
      "Test multinomial model accuracy: 0.5249999761581421\n"
     ]
    }
   ],
   "source": [
    "# use the model.evaluate function to report the loss and accuracy on the dev set\n",
    "score = model_multi.evaluate(np.array(X_test_multi), np.array(y_test), verbose=1)\n",
    "print('Test multinomial model loss:', score[0])\n",
    "print('Test multinomial model accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 10ms/step - loss: 0.6867 - accuracy: 0.6750\n",
      "Test binomial model loss: 0.6867132782936096\n",
      "Test binomial model accuracy: 0.675000011920929\n"
     ]
    }
   ],
   "source": [
    "score = model_bi.evaluate(np.array(X_test_bi), np.array(y_test), verbose=1)\n",
    "print('Test binomial model loss:', score[0])\n",
    "print('Test binomial model accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the same graph as with NB and LR, with your neural network model instead!\n",
    "# make sure to re-create your model each time you train it â you don't want to start with\n",
    "# an already trained network!\n",
    "\n",
    "# For a model with one hidden layer of 50 units:\n",
    "# Takes < 15 sec to run on Felix's computer w/ 3 epochs\n",
    "# Takes < 30 sec to run on Felix's computer w/ 10 epochs\n",
    "# Takes < 50 sec to run on Felix's computer w/ 20 epochs\n",
    "# you need not train your model more than 20 epochs\n",
    "# you should experiment with different numbers of epochs to see how performance varies\n",
    "# you need not create an experiment that takes > 10 min to run (please do not do this)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for multinomial model: 0.6885245901639345\n",
      "F1 Score for binomial model: 0.7085201793721974\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 Score for multinomial model: {f1_score(y_test, multi_assigned_labels)}')\n",
    "print(f'F1 Score for binomial model: {f1_score(y_test, bi_assigned_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the f1 scores for your model with the following settings, using the same number of epochs to train in both cases:\n",
    "- number of epochs used: 6 epochs\n",
    "- multinomial features: __YOUR ANSWER HERE__ \n",
    "- binarized features: __YOUR ANSWER HERE__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
