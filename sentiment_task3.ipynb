{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 3\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names & Sections\n",
    "----\n",
    "Names: Alec Condry (4120) and Shrihari Subramaniam (4120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Train a Logistic Regression Model (20 points)\n",
    "----\n",
    "\n",
    "Using `sklearn`'s implementation of `LogisticRegression`, conduct a similar analysis on the performance of a Logistic Regression classifier on the provided data set.\n",
    "\n",
    "Using the `time` module, you'll compare and contrast how long it takes your home-grown BoW vectorizing function vs. `sklearn`'s `CountVectorizer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hsubr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from collections import Counter\n",
    "import time\n",
    "import sentiment_utils as sutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\"\n",
    "\n",
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_tups = sutils.generate_tuples_from_file(DEV_FILE)\n",
    "\n",
    "# some variables you may want to use\n",
    "BINARIZED = False\n",
    "USE_COUNT_VECTORIZER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30705\n"
     ]
    }
   ],
   "source": [
    "# Write the functions needed (here or in sentiment_utils.py) to create vectorized BoW representations\n",
    "# of your data. We recommend starting with a multinomial BoW representation.\n",
    "# Each training example should be represented as a sparse vector.\n",
    "\n",
    "vocab = sutils.create_index(train_tups)\n",
    "\n",
    "reviews_train = train_tups[0]\n",
    "reviews_test = dev_tups[0]\n",
    "\n",
    "y_train = train_tups[1]\n",
    "y_test = dev_tups[1]\n",
    "\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took: 55.17911720275879 seconds\n"
     ]
    }
   ],
   "source": [
    "# how much time does it take to featurize the all data with your implementation?\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_train_multi = sutils.featurize(vocab=vocab, data_to_be_featurized_X=reviews_train, binary=BINARIZED, verbose=False)\n",
    "X_test_multi = sutils.featurize(vocab=vocab, data_to_be_featurized_X=reviews_test, binary=BINARIZED, verbose=False)\n",
    "\n",
    "X_train_bi = sutils.featurize(vocab=vocab, data_to_be_featurized_X=reviews_train, binary=not BINARIZED, verbose=False)\n",
    "X_test_bi = sutils.featurize(vocab=vocab, data_to_be_featurized_X=reviews_test, binary=not BINARIZED, verbose=False)\n",
    "\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# how much time does it take to featurize the all data with sklearn's CountVectorizer?\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "end = time.time()\n",
    "print(\"That took:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How big is your vocabulary using your vectorization function(s)? 30705\n",
    "2. How big is your vocabulary using the `CountVectorizer`? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average % of entries that are zeros in each vector: 99.51%\n"
     ]
    }
   ],
   "source": [
    "#  write any code you need analyze the relative sparsity of your vectorized representations of the data\n",
    "\n",
    "# YOUR CODE HERE\n",
    "percentages = []\n",
    "for i in X_train_bi:\n",
    "    counts = Counter(i)\n",
    "    percentage = counts[0] / (counts[0] + counts[1])\n",
    "    percentages.append(percentage * 100)\n",
    "\n",
    "\n",
    "# Print out the average % of entries that are zeros in each vector in the vectorized training data\n",
    "# YOUR CODE HERE\n",
    "average = sum(percentages)/len(percentages)\n",
    "print(f'Average % of entries that are zeros in each vector: {round(average, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the provided dev set, evaluate your model with precision, recall, and f1 score as well as accuracy\n",
    "# You may use nltk's implemented `precision`, `recall`, `f_measure`, and `accuracy` functions\n",
    "# (make sure to look at the documentation for these functions!)\n",
    "# you will be creating a similar graph for logistic regression and neural nets, so make sure\n",
    "# you use functions wisely so that you do not have excessive repeated code\n",
    "# write any helper functions you need in sentiment_utils.py (functions that you'll use in your other notebooks as well)\n",
    "\n",
    "\n",
    "# create a graph of your classifier's performance on the dev set as a function of the amount of training data\n",
    "# the x-axis should be the amount of training data (as a percentage of the total training data)\n",
    "# the y-axis should be the performance of the classifier on the dev set\n",
    "# the graph should have 4 lines, one for each of precision, recall, f1, and accuracy\n",
    "# the graph should have a legend, title, and axis labels\n",
    "\n",
    "# takes approx 30 sec on Felix's computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the following 4 combinations to determine which has the best final f1 score for your Logistic Regression model:\n",
    "- your vectorized features, multinomial: __enter your final f1 score here__\n",
    "- CountVectorizer features, multinomial: __enter your final f1 score here__\n",
    "- your vectorized features, binarized: __enter your final f1 score here__\n",
    "- CountVectorizer features, binarized: __enter your final f1 score here__\n",
    "\n",
    "Produce your graph(s) for the combination with the best final f1 score.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6120 REQUIRED\n",
    "----\n",
    "\n",
    "Find the top 100 most important features to your Logistic Regression classifier when using 100% of the training data. To access the weights of your model, you can access the `model.coef_` attribute. You'll want to use a `StandardScalar` preprocessor. This will help us deal with the fact that we expect counts of certain words to be higher (e.g. stop words).\n",
    "\n",
    "To find the importance of a feature, calculate the absolute value of each weight in the model, then order your features according to the absolute values of these weights. The feature with the heighest absolute value weight has the most importance.\n",
    "\n",
    "Use __your__ (not CountVectorizer) multinomial vectors for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# train a model on the scaled inputs\n",
    "# This takes Felix's computer about 6.5 sec to run\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the top 20 most informative features according to this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-evalaute your LR model with inputs that have been filtered to only use the top 500 most informative features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the same graph as before, but with the filtered inputs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
